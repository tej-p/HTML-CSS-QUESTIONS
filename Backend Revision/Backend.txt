Question ~ What is Caching? How can you save money with Caching?
Answer ~ 
In computing, a cache is a high-speed data storage layer which stores a subset of data,
typically transient in nature, so that future requests for that data are served up faster than is possible 
by accessing the data’s primary storage location. Caching allows you to efficiently reuse previously
retrieved or computed data.
Due to the high request rates or IOPS (Input/Output operations per second) supported by RAM and 
In-Memory engines, caching results in improved data retrieval performance and reduces cost at scale.
(https://aws.amazon.com/caching/#:~:text=A%20cache)



Question ~ What is load balancing?
Answer ~ 
Load balancing refers to efficiently distributing incoming network traffic across a group of 
backend servers, also known as a server farm or server pool.

Modern high‑traffic websites must serve hundreds of thousands, if not millions, of concurrent
requests from users or clients and return the correct text, images, video, or application data,
all in a fast and reliable manner. To cost‑effectively scale to meet these high volumes,
modern computing best practice generally requires adding more servers.

A load balancer acts as the “traffic cop” sitting in front of your servers and routing client requests
across all servers capable of fulfilling those requests in a manner that maximizes speed and capacity 
utilization and ensures that no one server is overworked, which could degrade performance. If a single 
server goes down, the load balancer redirects traffic to the remaining online servers. When a new server 
is added to the server group, the load balancer automatically starts to send requests to it.

In this manner, a load balancer performs the following functions :

    - Distributes client requests or network load efficiently across multiple servers
    - Ensures high availability and reliability by sending requests only to servers that are online
    - Provides the flexibility to add or subtract servers as demand dictates
(https://www.nginx.com/resources/glossary/load-balancing/)



Question ~ What is CAP Theorem?
Answer ~ 
The CAP theorem, originally introduced as the CAP principle, can be used to explain some of the competing 
requirements in a distributed system with replication. It is a tool used to make system designers aware of 
the trade-offs while designing networked shared-data systems. 

The three letters in CAP refer to three desirable properties of distributed systems with replicated data:
consistency - (among replicated copies), 
availability - of the system for read and write operations) and 
partition - tolerance in the face of the nodes in the system being partitioned by a network fault). 

The CAP theorem states that it is not possible to guarantee all three of the desirable properties – 
consistency, availability, and partition tolerance at the same time in a distributed system with data replication. 

The theorem states that networked shared-data systems can only strongly support two of the above three properties.
(https://www.geeksforgeeks.org/the-cap-theorem-in-dbms/)




Question ~ What is PACELC Theorem?
Answer ~ 
The PACELC theorem is an extension to the CAP theorem. Both theorems were developed to provide a 
framework for comparing distributed systems. Like the CAP theorem, the PACELC theorem states that in case of 
network partitioning in a distributed computer system, one has to choose between availability and consistency. 
PACELC extends the CAP theorem by introducing latency and consistency as additional attributes of distributed systems.
The theorem states that, “else, even when the system is running normally in the absence of partitions,
one has to choose between latency and consistency.”
(https://www.scylladb.com/glossary/pacelc-theorem/)




Question ~ What is Eventual Consistency?
Answer ~ 
Eventual Consistency is a guarantee that when an update is made in a distributed database,
that update will eventually be reflected in all nodes that store the data, resulting in the same response 
every time the data is queried.
Consistency refers to a database query returning the same data each time the same request is made. 
Strong consistency means the latest data is returned, but, due to internal consistency methods, it may result 
with higher latency or delay. With eventual consistency, results are less consistent early on, but they 
are provided much faster with low latency. Early results of eventual consistency data queries may not have 
the most recent updates because it takes time for updates to reach replicas across a database cluster.
(https://www.scylladb.com/glossary/eventual-consistency/)




Question ~ What is Strong Consistency?
Answer ~ 




Question ~ What are the different types of databases?
Answer ~ 




Question ~ What are message queues?
Answer ~ 




Question ~ Which service by Amazon Web Services (AWS) can you use for Queues?
Answer ~ 




Question ~ What is Pub Sub ?
Answer ~ 




Question ~ What are webhooks?
Answer ~ 




Question ~ What is Docker? Why do we use it?
Answer ~ 




Question ~ What is S3 Service in AWS?
Answer ~ 




Question ~ What is EC2 Instance in AWS?
Answer ~ 




Question ~ What is Cloudfront in AWS?
Answer ~ 




Question ~ What is Route 53 In AWS?
Answer ~ 




Question ~ What are ELBs in AWS?
Answer ~ 




Question ~ What is TLS?
Answer ~ 




Question ~ What is t
Answer ~ 

he difference HTTPS vs HTTP?


Question ~ What is a reverse proxy?Answer ~ 